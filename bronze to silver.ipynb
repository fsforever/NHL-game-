{"cells":[{"cell_type":"markdown","source":["# game.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f36b11b3-95f2-4c49-929a-f5139deabb29"},{"cell_type":"markdown","source":["Performed column standardization to snake_case, removed duplicates, dropped columns with over 85% null values (except key identifiers), validated score columns for non-negative values, and verified row counts before and after loading to ensure consistency."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba20b46d-1916-454d-9060-a0db3e00d040"},{"cell_type":"code","source":["\n","# ============================\n","# Game Pipeline\n","# ============================\n","\n","import pandas as pd\n","import re\n","\n","df_game = pd.read_csv(\"/lakehouse/default/Files/dataset/game.csv\")\n","\n","# data inspection \n","\n","print(df_game.head())\n","print(df_game.info())\n","\n","# Step 1 â€” Standardize column names to snake_case\n","def to_snake_case(name):\n","    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n","    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n","    return name.lower()\n","\n","df_game.columns = [to_snake_case(c) for c in df_game.columns]\n","\n","# Step 2 â€” Drop columns with >85% null values but keep required columns\n","required_cols = ['game_id', 'season', 'home_team_id', 'away_team_id']\n","null_threshold = 0.85\n","df_game = df_game.drop(columns=[c for c in df_game.columns \n","                                if df_game[c].isnull().mean() > null_threshold and c not in required_cols])\n","print(\"âœ… Dropped high-null columns where applicable.\")\n","\n","# Step 3 â€” Remove duplicates\n","df_game = df_game.drop_duplicates()\n","\n","# Step 4 â€” Range/value validation\n","if 'home_score' in df_game.columns:\n","    invalid_home = df_game[df_game['home_goals'] < 0]\n","    if not invalid_home.empty:\n","        print(\"âš ï¸ Invalid home_score rows:\")\n","        print(invalid_home)\n","\n","if 'away_score' in df_game.columns:\n","    invalid_away = df_game[df_game['away_goals'] < 0]\n","    if not invalid_away.empty:\n","        print(\"âš ï¸ Invalid away_score rows:\")\n","        print(invalid_away)\n","\n","# Step 5 â€” Save to Silver layer\n","sdf_game = spark.createDataFrame(df_game)\n","sdf_game.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver_game\")\n","\n","# Step 6 â€” Verify row count\n","df_loaded_game = spark.read.table(\"silver_game\").toPandas()\n","print(f\"âœ… Rows before load: {len(df_game)}, after load: {len(df_loaded_game)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"c290fbeb-1be0-4cc5-aeca-d29437c557f5","normalized_state":"finished","queued_time":"2025-10-19T08:47:49.959539Z","session_start_time":null,"execution_start_time":"2025-10-19T08:47:49.9607568Z","execution_finish_time":"2025-10-19T08:48:38.9720914Z","parent_msg_id":"f5a93d8e-3122-4862-8c7d-cd009ad26f37"},"text/plain":"StatementMeta(, c290fbeb-1be0-4cc5-aeca-d29437c557f5, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["      game_id    season type         date_time_GMT  away_team_id  \\\n0  2016020045  20162017    R  2016-10-19T00:30:00Z             4   \n1  2017020812  20172018    R  2018-02-07T00:00:00Z            24   \n2  2015020314  20152016    R  2015-11-24T01:00:00Z            21   \n3  2015020849  20152016    R  2016-02-17T00:00:00Z            52   \n4  2017020586  20172018    R  2017-12-30T03:00:00Z            20   \n\n   home_team_id  away_goals  home_goals       outcome home_rink_side_start  \\\n0            16           4           7  home win REG                right   \n1             7           4           3   away win OT                 left   \n2            52           4           1  away win REG                right   \n3            12           1           2  home win REG                right   \n4            24           1           2  home win REG                 left   \n\n            venue           venue_link   venue_time_zone_id  \\\n0   United Center  /api/v1/venues/null      America/Chicago   \n1  KeyBank Center  /api/v1/venues/null     America/New_York   \n2      MTS Centre  /api/v1/venues/null     America/Winnipeg   \n3       PNC Arena  /api/v1/venues/null     America/New_York   \n4    Honda Center  /api/v1/venues/null  America/Los_Angeles   \n\n   venue_time_zone_offset venue_time_zone_tz  \n0                      -5                CDT  \n1                      -4                EDT  \n2                      -5                CDT  \n3                      -4                EDT  \n4                      -7                PDT  \n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 26305 entries, 0 to 26304\nData columns (total 15 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   game_id                 26305 non-null  int64 \n 1   season                  26305 non-null  int64 \n 2   type                    26305 non-null  object\n 3   date_time_GMT           26305 non-null  object\n 4   away_team_id            26305 non-null  int64 \n 5   home_team_id            26305 non-null  int64 \n 6   away_goals              26305 non-null  int64 \n 7   home_goals              26305 non-null  int64 \n 8   outcome                 26305 non-null  object\n 9   home_rink_side_start    25109 non-null  object\n 10  venue                   26305 non-null  object\n 11  venue_link              26305 non-null  object\n 12  venue_time_zone_id      26305 non-null  object\n 13  venue_time_zone_offset  26305 non-null  int64 \n 14  venue_time_zone_tz      26305 non-null  object\ndtypes: int64(7), object(8)\nmemory usage: 3.0+ MB\nNone\nâœ… Dropped high-null columns where applicable.\nâœ… Rows before load: 23735, after load: 23735\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d23be180-c3bf-4340-b7ac-c4775a157046"},{"cell_type":"markdown","source":["# game_skater_stats.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d452add-9f5e-468e-8ed5-53e8bdd81214"},{"cell_type":"markdown","source":["Standardized column names to snake_case, dropped columns exceeding 85% null values, removed duplicates, renamed penaltyMinutes to pim, validated numeric columns such as goals and time_on_ice, and verified schema and row count consistency before and after loading."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ccb214b-9472-4ced-8130-15e84b5493a8"},{"cell_type":"code","source":["# Step 1a â€” Read raw data\n","skater_file = \"/lakehouse/default/Files/dataset/game_skater_stats.csv\"\n","df_skater_raw = pd.read_csv(skater_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_skater_raw)}\")\n","\n","# step 1b - data inspection \n","print(df_skater_raw.head())\n","print(df_skater_raw.info())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"c290fbeb-1be0-4cc5-aeca-d29437c557f5","normalized_state":"finished","queued_time":"2025-10-19T08:50:01.1979587Z","session_start_time":null,"execution_start_time":"2025-10-19T08:50:01.1990915Z","execution_finish_time":"2025-10-19T08:50:03.6087187Z","parent_msg_id":"bae4e246-a123-4b93-b7cd-4e1c3241db67"},"text/plain":"StatementMeta(, c290fbeb-1be0-4cc5-aeca-d29437c557f5, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“¥ Raw rows loaded: 945830\n      game_id  player_id  team_id  timeOnIce  assists  goals  shots  hits  \\\n0  2016020045    8468513        4        955        1      0      0   2.0   \n1  2016020045    8476906        4       1396        1      0      4   2.0   \n2  2016020045    8474668        4        915        0      0      1   1.0   \n3  2016020045    8473512        4       1367        3      0      0   0.0   \n4  2016020045    8471762        4        676        0      0      3   2.0   \n\n   powerPlayGoals  powerPlayAssists  ...  faceoffTaken  takeaways  giveaways  \\\n0               0                 0  ...             0        1.0        1.0   \n1               0                 0  ...             0        1.0        2.0   \n2               0                 0  ...             0        2.0        0.0   \n3               0                 2  ...            27        0.0        0.0   \n4               0                 0  ...             0        0.0        1.0   \n\n   shortHandedGoals  shortHandedAssists  blocked  plusMinus  evenTimeOnIce  \\\n0                 0                   0      1.0          1            858   \n1                 0                   0      2.0          0           1177   \n2                 0                   0      0.0         -1            805   \n3                 0                   0      0.0         -1           1083   \n4                 0                   0      0.0         -1            613   \n\n   shortHandedTimeOnIce  powerPlayTimeOnIce  \n0                    97                   0  \n1                     0                 219  \n2                     0                 110  \n3                    19                 265  \n4                    63                   0  \n\n[5 rows x 22 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 945830 entries, 0 to 945829\nData columns (total 22 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   game_id               945830 non-null  int64  \n 1   player_id             945830 non-null  int64  \n 2   team_id               945830 non-null  int64  \n 3   timeOnIce             945830 non-null  int64  \n 4   assists               945830 non-null  int64  \n 5   goals                 945830 non-null  int64  \n 6   shots                 945830 non-null  int64  \n 7   hits                  547723 non-null  float64\n 8   powerPlayGoals        945830 non-null  int64  \n 9   powerPlayAssists      945830 non-null  int64  \n 10  penaltyMinutes        945830 non-null  int64  \n 11  faceOffWins           945830 non-null  int64  \n 12  faceoffTaken          945830 non-null  int64  \n 13  takeaways             547723 non-null  float64\n 14  giveaways             547723 non-null  float64\n 15  shortHandedGoals      945830 non-null  int64  \n 16  shortHandedAssists    945830 non-null  int64  \n 17  blocked               547723 non-null  float64\n 18  plusMinus             945830 non-null  int64  \n 19  evenTimeOnIce         945830 non-null  int64  \n 20  shortHandedTimeOnIce  945830 non-null  int64  \n 21  powerPlayTimeOnIce    945830 non-null  int64  \ndtypes: float64(4), int64(18)\nmemory usage: 158.8 MB\nNone\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cccd3778-bca6-4a22-9851-3abccf1e4faf"},{"cell_type":"code","source":["# ============================\n","# Game Skater Stats Pipeline\n","# ============================\n","\n","import pandas as pd\n","import re\n","\n","# Step 1 â€” Read raw data\n","skater_file = \"/lakehouse/default/Files/dataset/game_skater_stats.csv\"\n","df_skater_raw = pd.read_csv(skater_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_skater_raw)}\")\n","\n","\n","# Step 2 â€” Define required columns to never drop\n","required_columns = ['game_id', 'player_id', 'team_id', 'time_on_ice', 'assists', 'goals']\n","\n","# Step 3 â€” Drop columns with >85% null values but keep required columns\n","null_threshold = 0.85\n","null_ratios = df_skater_raw.isnull().mean()\n","cols_to_drop = [col for col in df_skater_raw.columns\n","                if null_ratios[col] > null_threshold and col not in required_columns]\n","\n","if cols_to_drop:\n","    df_skater = df_skater_raw.drop(columns=cols_to_drop)\n","    print(f\"ðŸ§¹ Dropped columns with >{int(null_threshold*100)}% nulls: {cols_to_drop}\")\n","else:\n","    df_skater = df_skater_raw.copy()\n","    print(f\"âœ… No columns exceeded {int(null_threshold*100)}% null threshold.\")\n","\n","# Step 4 â€” Validate numeric columns\n","skater_checks = {\n","    'goals': lambda x: x >= 0,\n","    'TimeOnIce': lambda x: x >= 0  # Use original CSV column name for validation\n","}\n","\n","for col, rule in skater_checks.items():\n","    if col in df_skater.columns:\n","        invalid = df_skater[~df_skater[col].apply(rule)]\n","        if len(invalid) > 0:\n","            print(f\"âš ï¸ Invalid values in {col}: {len(invalid)} rows\")\n","        else:\n","            print(f\"âœ… All values valid in {col}\")\n","\n","# Step 5 â€” Remove duplicates based on key identifiers\n","unique_id_cols = ['game_id', 'player_id']\n","before = len(df_skater)\n","df_skater = df_skater.drop_duplicates(subset=unique_id_cols)\n","print(f\"âœ… Removed {before - len(df_skater)} duplicate rows\")\n","\n","# Step 6 â€” Convert only non-snake_case columns to snake_case\n","def is_snake_case(name):\n","    return name == name.lower() and \"_\" in name\n","\n","def to_snake_case(name):\n","    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n","    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n","    return name.lower()\n","\n","df_skater.columns = [to_snake_case(col) if not is_snake_case(col) else col for col in df_skater.columns]\n","\n","# Step 6b â€” Rename penaltyMinutes to pim if present\n","if 'penalty_minutes' in df_skater.columns:\n","    df_skater = df_skater.rename(columns={'penalty_minutes': 'pim'})\n","\n","# Optional: enforce any known column corrections (like face_off_taken)\n","rename_map = {\n","    'faceoff_taken': 'face_off_taken'\n","}\n","df_skater = df_skater.rename(columns={k: v for k, v in rename_map.items() if k in df_skater.columns})\n","\n","print(\"âœ… Column names standardized; existing snake_case preserved; penaltyMinutes renamed to pim.\")\n","\n","# Step 7 â€” Preserve column order using renamed columns\n","original_sequence = [col for col in df_skater.columns if col in df_skater.columns]\n","df_skater = df_skater[original_sequence]\n","\n","print(f\"âœ… Columns kept for Silver table: {list(df_skater.columns)}\")\n","\n","# Step 8 â€” Verify required columns exist (for warning, not selection)\n","missing_cols = [col for col in required_columns if col not in df_skater.columns]\n","if missing_cols:\n","    print(f\"âš ï¸ Missing required columns: {missing_cols}\")\n","else:\n","    print(\"âœ… All required columns present before load.\")\n","\n","print(f\"ðŸ“Š Final row count before load: {len(df_skater)}\")\n","\n","# Step 9 â€” Load cleaned data to Silver Layer with schema overwrite\n","sdf_skater = spark.createDataFrame(df_skater)\n","sdf_skater.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .saveAsTable(\"silver_game_skater_stats\")\n","print(\"âœ… Data saved to silver_game_skater_stats\")\n","\n","# Step 10 â€” Verify after load\n","df_loaded = spark.read.table(\"silver_game_skater_stats\").toPandas()\n","\n","# Compare row count\n","if len(df_loaded) == len(df_skater):\n","    print(\"âœ… Row count matches after load.\")\n","else:\n","    print(f\"âŒ Row count mismatch! Before: {len(df_skater)}, After: {len(df_loaded)}\")\n","\n","# Compare columns\n","if set(df_loaded.columns) == set(df_skater.columns):\n","    print(\"âœ… Column structure matches after load.\")\n","else:\n","    print(\"âŒ Column mismatch detected after load.\")\n","\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"8c4d41ea-f1b4-470d-96bb-8c3981bacd28","normalized_state":"finished","queued_time":"2025-10-18T07:17:22.2158753Z","session_start_time":null,"execution_start_time":"2025-10-18T07:17:22.217096Z","execution_finish_time":"2025-10-18T07:17:37.0650181Z","parent_msg_id":"d89f8866-880d-4279-9a9c-61249ec26a28"},"text/plain":"StatementMeta(, 8c4d41ea-f1b4-470d-96bb-8c3981bacd28, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“¥ Raw rows loaded: 945830\nâœ… No columns exceeded 85% null threshold.\nâœ… All values valid in goals\nâœ… Removed 92426 duplicate rows\nâœ… Column names standardized; existing snake_case preserved; penaltyMinutes renamed to pim.\nâœ… Columns kept for Silver table: ['game_id', 'player_id', 'team_id', 'time_on_ice', 'assists', 'goals', 'shots', 'hits', 'power_play_goals', 'power_play_assists', 'pim', 'face_off_wins', 'face_off_taken', 'takeaways', 'giveaways', 'short_handed_goals', 'short_handed_assists', 'blocked', 'plus_minus', 'even_time_on_ice', 'short_handed_time_on_ice', 'power_play_time_on_ice']\nâœ… All required columns present before load.\nðŸ“Š Final row count before load: 853404\nâœ… Data saved to silver_game_skater_stats\nâœ… Row count matches after load.\nâœ… Column structure matches after load.\n"]}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee17df02-b815-4160-bbf5-a06b6b0e1775"},{"cell_type":"markdown","source":["# game_goalie_stats.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c28c007d-88db-4461-a4ae-676024e66910"},{"cell_type":"markdown","source":["Cleaned and standardized columns, removed duplicates, dropped sparse columns, validated shot- and save-related metrics, converted column names to consistent snake_case, and confirmed data integrity through schema and row count checks post-load."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ecec567a-35d3-41a0-ab8d-98fdc4d22c03"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Step 1a â€” Read raw data\n","goalie_file = \"/lakehouse/default/Files/dataset/game_goalie_stats.csv\"\n","df_goalie_raw = pd.read_csv(goalie_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_goalie_raw)}\")\n","\n","# Step 1b - data inspection \n","print(df_goalie_raw.head())\n","print(df_goalie_raw.info())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"06fc82b9-0680-4a26-9393-7ef8e169a9ec"},{"cell_type":"code","source":["# ============================\n","# Game Goalie Stats Pipeline\n","# ============================\n","\n","import pandas as pd\n","import re\n","\n","# Step 1 â€” Read raw data\n","goalie_file = \"/lakehouse/default/Files/dataset/game_goalie_stats.csv\"\n","df_goalie_raw = pd.read_csv(goalie_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_goalie_raw)}\")\n","\n","# Step 2 â€” Define required columns to never drop\n","required_columns = ['game_id', 'player_id', 'team_id', 'shots', 'saves']\n","\n","# Step 3 â€” Drop columns with >85% null values but keep required columns\n","null_threshold = 0.85\n","null_ratios = df_goalie_raw.isnull().mean()\n","cols_to_drop = [col for col in df_goalie_raw.columns\n","                if null_ratios[col] > null_threshold and col not in required_columns]\n","\n","if cols_to_drop:\n","    df_goalie = df_goalie_raw.drop(columns=cols_to_drop)\n","    print(f\"ðŸ§¹ Dropped columns with >{int(null_threshold*100)}% nulls: {cols_to_drop}\")\n","else:\n","    df_goalie = df_goalie_raw.copy()\n","    print(f\"âœ… No columns exceeded {int(null_threshold*100)}% null threshold.\")\n","\n","# Step 4 â€” Validate numeric columns\n","goalie_checks = {\n","    'saves': lambda x: x >= 0,\n","    'shots': lambda x: x >= 0\n","}\n","\n","for col, rule in goalie_checks.items():\n","    if col in df_goalie.columns:\n","        invalid = df_goalie[~df_goalie[col].apply(rule)]\n","        if len(invalid) > 0:\n","            print(f\"âš ï¸ Invalid values in {col}: {len(invalid)} rows\")\n","        else:\n","            print(f\"âœ… All values valid in {col}\")\n","\n","# Step 5 â€” Remove duplicates based on key identifiers\n","unique_id_cols = ['game_id', 'player_id']\n","before = len(df_goalie)\n","df_goalie = df_goalie.drop_duplicates(subset=unique_id_cols)\n","print(f\"âœ… Removed {before - len(df_goalie)} duplicate rows\")\n","\n","# Step 6 â€” Convert only non-snake_case columns to snake_case\n","def is_snake_case(name):\n","    return name == name.lower() and \"_\" in name\n","\n","def to_snake_case(name):\n","    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n","    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n","    return name.lower()\n","\n","df_goalie.columns = [to_snake_case(col) if not is_snake_case(col) else col for col in df_goalie.columns]\n","\n","# Step 6b â€” Rename penaltyMinutes to pim if present\n","if 'penalty_minutes' in df_goalie.columns:\n","    df_goalie = df_goalie.rename(columns={'penalty_minutes': 'pim'})\n","\n","# Optional: enforce any known column corrections if needed\n","rename_map = {\n","    # Ensure save_pct stays snake_case if exists\n","    'savePct': 'save_pct'\n","}\n","df_goalie = df_goalie.rename(columns={k: v for k, v in rename_map.items() if k in df_goalie.columns})\n","\n","print(\"âœ… Column names standardized; existing snake_case preserved; penaltyMinutes renamed to pim.\")\n","\n","# Step 7 â€” Verify required columns exist (warning only, not subsetting)\n","missing_cols = [col for col in required_columns if col not in df_goalie.columns]\n","if missing_cols:\n","    print(f\"âš ï¸ Missing required columns: {missing_cols}\")\n","else:\n","    print(\"âœ… All required columns present before load.\")\n","\n","print(f\"ðŸ“Š Final row count before load: {len(df_goalie)}\")\n","\n","# Step 8 â€” Load cleaned data to Silver Layer with schema overwrite\n","sdf_goalie = spark.createDataFrame(df_goalie)\n","sdf_goalie.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .saveAsTable(\"silver_game_goalie_stats\")\n","print(\"âœ… Data saved to silver_game_goalie_stats\")\n","\n","# Step 9 â€” Verify after load\n","df_loaded = spark.read.table(\"silver_game_goalie_stats\").toPandas()\n","\n","# Compare row count\n","if len(df_loaded) == len(df_goalie):\n","    print(\"âœ… Row count matches after load.\")\n","else:\n","    print(f\"âŒ Row count mismatch! Before: {len(df_goalie)}, After: {len(df_loaded)}\")\n","\n","# Compare columns\n","if set(df_loaded.columns) == set(df_goalie.columns):\n","    print(\"âœ… Column structure matches after load.\")\n","else:\n","    print(\"âŒ Column mismatch detected after load.\")\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"8c4d41ea-f1b4-470d-96bb-8c3981bacd28","normalized_state":"finished","queued_time":"2025-10-18T07:24:27.8348273Z","session_start_time":null,"execution_start_time":"2025-10-18T07:24:27.8360232Z","execution_finish_time":"2025-10-18T07:24:39.9057659Z","parent_msg_id":"32951444-7561-4859-8468-0b7206a53bc4"},"text/plain":"StatementMeta(, 8c4d41ea-f1b4-470d-96bb-8c3981bacd28, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“¥ Raw rows loaded: 56656\nâœ… No columns exceeded 85% null threshold.\nâœ… All values valid in saves\nâœ… All values valid in shots\nâœ… Removed 5493 duplicate rows\nâœ… Column names standardized; existing snake_case preserved; penaltyMinutes renamed to pim.\nâœ… All required columns present before load.\nðŸ“Š Final row count before load: 51163\nâœ… Data saved to silver_game_goalie_stats\nâœ… Row count matches after load.\nâœ… Column structure matches after load.\n"]}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f2aa47b-11e2-4d66-b47e-d7ad9b6ed83f"},{"cell_type":"markdown","source":["# game_team_stats.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6294389-df80-45e9-b929-a6a7ddca22a7"},{"cell_type":"markdown","source":["Cleaned duplicate entries, standardized column naming, removed sparse columns, validated team-level game statistics (goals, shots, PIM), and verified schema alignment between the raw and Silver layers."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3c5b9647-e6c9-4eb8-94c3-89c66ee744f9"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Step 1a â€” Load raw CSV\n","team_file = \"/lakehouse/default/Files/dataset/game_teams_stats.csv\"\n","df_team_raw = pd.read_csv(team_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_team_raw)}\")\n","\n","# Step 1b - data inspection \n","print(df_team_raw.head())\n","print(df_team_raw.info())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fcd17d90-f0b7-44f4-ac51-778161848918"},{"cell_type":"code","source":["# ============================\n","# Game Team Stats Pipeline\n","# ============================\n","\n","import pandas as pd\n","import re\n","\n","# Step 1 â€” Load raw CSV\n","team_file = \"/lakehouse/default/Files/dataset/game_teams_stats.csv\"\n","df_team_raw = pd.read_csv(team_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_team_raw)}\")\n","\n","# Step 2 â€” Required columns (never drop)\n","required_columns = ['game_id', 'team_id', 'goals', 'shots', 'pim']\n","\n","# Step 3 â€” Drop columns with >85% nulls, excluding required columns\n","null_threshold = 0.85\n","null_ratios = df_team_raw.isnull().mean()\n","cols_to_drop = [col for col in df_team_raw.columns if null_ratios[col] > null_threshold and col not in required_columns]\n","\n","if cols_to_drop:\n","    df_team = df_team_raw.drop(columns=cols_to_drop)\n","    print(f\"ðŸ§¹ Dropped columns with >{int(null_threshold*100)}% nulls: {cols_to_drop}\")\n","else:\n","    df_team = df_team_raw.copy()\n","    print(f\"âœ… No columns exceeded {int(null_threshold*100)}% null threshold.\")\n","\n","# Step 4 â€” Standardize column names to snake_case\n","def is_snake_case(name):\n","    return name == name.lower() and \"_\" in name\n","\n","def to_snake_case(name):\n","    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n","    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n","    return name.lower()\n","\n","df_team.columns = [to_snake_case(col) if not is_snake_case(col) else col for col in df_team.columns]\n","\n","# Step 4b â€” Rename penaltyMinutes to pim if present\n","if 'penaltyminutes' in df_team.columns:\n","    df_team = df_team.rename(columns={'penaltyminutes': 'pim'})\n","\n","print(\"âœ… Column names standardized; existing snake_case preserved; penaltyMinutes renamed to pim.\")\n","\n","# Step 5 â€” Validate numeric columns and display invalid rows\n","numeric_checks = ['goals', 'shots', 'pim']\n","invalid_rows = df_team[\n","    (df_team['goals'] < 0) |\n","    (df_team['shots'] < 0) |\n","    (df_team['pim'] < 0)\n","]\n","\n","if not invalid_rows.empty:\n","    print(f\"âš ï¸ Found {len(invalid_rows)} invalid rows:\")\n","    print(invalid_rows.to_string(index=False))\n","    \n","    # Optional: save invalid rows for audit\n","    invalid_rows.to_csv(\"/lakehouse/default/Files/invalid_game_team_stats.csv\", index=False)\n","    \n","    # Remove invalid rows from main DataFrame\n","    df_team = df_team.drop(invalid_rows.index)\n","    print(f\"âœ… Removed {len(invalid_rows)} invalid rows\")\n","else:\n","    print(\"âœ… No invalid numeric rows found\")\n","\n","# Step 6 â€” Remove duplicates based on key identifiers\n","unique_id_cols = ['game_id', 'team_id']\n","before = len(df_team)\n","df_team = df_team.drop_duplicates(subset=unique_id_cols)\n","print(f\"âœ… Removed {before - len(df_team)} duplicate rows\")\n","\n","# Step 7 â€” Verify required columns exist\n","missing_cols = [col for col in required_columns if col not in df_team.columns]\n","if missing_cols:\n","    print(f\"âš ï¸ Missing required columns: {missing_cols}\")\n","else:\n","    print(\"âœ… All required columns present before load.\")\n","\n","print(f\"ðŸ“Š Final row count before load: {len(df_team)}\")\n","\n","# Step 8 â€” Load cleaned data to Silver Layer with schema overwrite\n","sdf_team = spark.createDataFrame(df_team)\n","sdf_team.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .saveAsTable(\"silver_game_team_stats\")\n","print(\"âœ… Data saved to silver_game_team_stats\")\n","\n","# Step 9 â€” Verify after load\n","df_loaded = spark.read.table(\"silver_game_team_stats\").toPandas()\n","\n","# Compare row count\n","if len(df_loaded) == len(df_team):\n","    print(\"âœ… Row count matches after load.\")\n","else:\n","    print(f\"âŒ Row count mismatch! Before: {len(df_team)}, After: {len(df_loaded)}\")\n","\n","# Compare columns\n","if list(df_loaded.columns) == list(df_team.columns):\n","    print(\"âœ… Column order and names match after load.\")\n","else:\n","    print(\"âŒ Column mismatch detected after load.\")\n","\n","\n","\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":38,"statement_ids":[38],"state":"finished","livy_statement_state":"available","session_id":"8c4d41ea-f1b4-470d-96bb-8c3981bacd28","normalized_state":"finished","queued_time":"2025-10-18T08:01:30.629825Z","session_start_time":null,"execution_start_time":"2025-10-18T08:01:30.6310863Z","execution_finish_time":"2025-10-18T08:01:38.6058641Z","parent_msg_id":"01d95e1c-48e6-44ab-bce6-b0059826058b"},"text/plain":"StatementMeta(, 8c4d41ea-f1b4-470d-96bb-8c3981bacd28, 38, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“¥ Raw rows loaded: 52610\nâœ… No columns exceeded 85% null threshold.\nâœ… Column names standardized; existing snake_case preserved; penaltyMinutes renamed to pim.\nâœ… No invalid numeric rows found\nâœ… Removed 5140 duplicate rows\nâœ… All required columns present before load.\nðŸ“Š Final row count before load: 47470\nâœ… Data saved to silver_game_team_stats\nâœ… Row count matches after load.\nâœ… Column order and names match after load.\n"]}],"execution_count":36,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f293d0af-bc48-430f-a025-dab7cac4af16"},{"cell_type":"markdown","source":["# player_info.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11f17af7-513f-4a84-ab72-0dc822edee81"},{"cell_type":"markdown","source":["Converted height (inches) and weight (pounds) to metric units with standardized precision, handled missing values via median and mode imputation, validated categorical fields like shooting hand and position, standardized string formatting (names, cities, countries), and ensured clean, consistent data ready for analytical joins."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7b07c821-d632-4c65-afd2-c315c24c8a8c"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Step 1 â€” Load raw CSV\n","player_file = \"/lakehouse/default/Files/dataset/player_info.csv\"\n","df_player = pd.read_csv(player_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_player)}\")\n","\n","# Step 1b - data inspection \n","print(df_player.head())\n","print(df_player.info())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0cf9ef7b-ff2e-49a0-aa67-55f89607ca05"},{"cell_type":"code","source":["\n","# ============================\n","# player_info Pipeline\n","# ============================\n","\n","import pandas as pd\n","import re\n","\n","# Step 1 â€” Load raw CSV\n","player_file = \"/lakehouse/default/Files/dataset/player_info.csv\"\n","df_player = pd.read_csv(player_file)\n","print(f\"ðŸ“¥ Raw rows loaded: {len(df_player)}\")\n","\n","# Step 2 â€” Standardize column names to snake_case\n","def to_snake_case(name):\n","    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n","    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n","    return name.lower()\n","\n","df_player.columns = [to_snake_case(c) if not (c==c.lower() and \"_\" in c) else c for c in df_player.columns]\n","print(\"âœ… Column names standardized; existing snake_case preserved.\")\n","\n","# Step 3 â€” Drop columns with >85% nulls (keep required)\n","required_columns = ['player_id', 'first_name', 'last_name', 'birth_date', 'nationality']\n","null_threshold = 0.85\n","df_player = df_player.drop(columns=[c for c in df_player.columns \n","                                    if df_player[c].isnull().mean() > null_threshold \n","                                    and c not in required_columns])\n","print(\"âœ… Dropped high-null columns where applicable.\")\n","\n","# Step 4 â€” Convert weight to kg, round to 2 dp, drop height in inches, round height_cm\n","if 'weight' in df_player.columns:\n","    df_player['weight'] = (df_player['weight'] * 0.453592).round(2)\n","    print(\"âœ… Converted weight to kg (2 decimals)\")\n","if 'height_cm' in df_player.columns:\n","    df_player['height_cm'] = df_player['height_cm'].round(2)\n","if 'height' in df_player.columns:\n","    df_player = df_player.drop(columns=['height'])\n","    print(\"âœ… Dropped original height (inches); height_cm preserved\")\n","\n","# Step 5 â€” Range/value checks\n","numeric_ranges = {'height_cm': (140, 230), 'weight': (40, 150)}  # height in cm, weight in kg\n","for col, (min_val, max_val) in numeric_ranges.items():\n","    if col in df_player.columns:\n","        invalid = df_player[(df_player[col] < min_val) | (df_player[col] > max_val)]\n","        if not invalid.empty:\n","            print(f\"âš ï¸ Invalid {col} values:\")\n","            print(invalid[['player_id', col]])\n","        else:\n","            print(f\"âœ… All {col} values valid\")\n","\n","# Step 6 â€” Categorical validation\n","if 'shoots_catches' in df_player.columns:\n","    invalid = df_player[~df_player['shoots_catches'].isin(['L','R']) & df_player['shoots_catches'].notna()]\n","    if not invalid.empty:\n","        print(f\"âš ï¸ Invalid shoots_catches values:\\n{invalid[['player_id','shoots_catches']]}\")\n","if 'primary_position' in df_player.columns:\n","    valid_pos = ['C','LW','RW','D','G']\n","    invalid = df_player[~df_player['primary_position'].isin(valid_pos)]\n","    if not invalid.empty:\n","        print(f\"âš ï¸ Invalid primary_position values:\\n{invalid[['player_id','primary_position']]}\")\n","\n","# Step 7 â€” Standardize text formatting\n","for col in ['first_name','last_name','birth_city']:\n","    if col in df_player.columns:\n","        df_player[col] = df_player[col].str.title()\n","for col in ['country','birth_state_province']:\n","    if col in df_player.columns:\n","        df_player[col] = df_player[col].str.upper()\n","\n","# Step 8 â€” Handle missing values\n","# Numeric: fill with median\n","for col in ['height_cm','weight']:\n","    if col in df_player.columns:\n","        df_player[col].fillna(df_player[col].median(), inplace=True)\n","# Categorical: fill with mode\n","for col in ['shoots_catches']:\n","    if col in df_player.columns:\n","        df_player[col].fillna(df_player[col].mode()[0], inplace=True)\n","\n","# Step 9 â€” Remove duplicates\n","df_player = df_player.drop_duplicates(subset=['player_id'])\n","print(f\"âœ… Removed duplicates; final rows: {len(df_player)}\")\n","\n","# Step 10 â€” Verification before load\n","missing_cols = [col for col in required_columns if col not in df_player.columns]\n","if missing_cols:\n","    print(f\"âš ï¸ Missing required columns before load: {missing_cols}\")\n","else:\n","    print(\"âœ… All required columns present before load.\")\n","print(f\"ðŸ“Š Final row count before load: {len(df_player)}\")\n","\n","# Step 11 â€” Save to Silver Layer\n","spark.createDataFrame(df_player).write.mode(\"overwrite\") \\\n","      .option(\"overwriteSchema\", \"true\").format(\"delta\") \\\n","      .saveAsTable(\"silver_player_info\")\n","print(\"âœ… Data saved to silver_player_info\")\n","\n","# Step 12 â€” Verification after load\n","df_loaded = spark.read.table(\"silver_player_info\").toPandas()\n","missing_cols_after = [col for col in required_columns if col not in df_loaded.columns]\n","if missing_cols_after:\n","    print(f\"âš ï¸ Missing required columns after load: {missing_cols_after}\")\n","else:\n","    print(\"âœ… All required columns present after load.\")\n","if len(df_loaded) == len(df_player):\n","    print(f\"âœ… Row count matches after load: {len(df_loaded)}\")\n","else:\n","    print(f\"âš ï¸ Row count mismatch! Loaded: {len(df_loaded)}, Expected: {len(df_player)}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"57bc7e90-2a29-4db4-9fec-ae5f7b4d21a2","normalized_state":"finished","queued_time":"2025-10-18T09:04:10.6585733Z","session_start_time":null,"execution_start_time":"2025-10-18T09:04:10.6598846Z","execution_finish_time":"2025-10-18T09:04:23.0118541Z","parent_msg_id":"6b3794a5-08b0-4220-80cc-b41d68ab209b"},"text/plain":"StatementMeta(, 57bc7e90-2a29-4db4-9fec-ae5f7b4d21a2, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["ðŸ“¥ Raw rows loaded: 3925\nâœ… Column names standardized; existing snake_case preserved.\nâœ… Dropped high-null columns where applicable.\nâœ… Converted weight to kg (2 decimals)\nâœ… Dropped original height (inches); height_cm preserved\nâœ… All height_cm values valid\nâœ… All weight values valid\nâœ… Removed duplicates; final rows: 3925\nâœ… All required columns present before load.\nðŸ“Š Final row count before load: 3925\nâœ… Data saved to silver_player_info\nâœ… All required columns present after load.\nâœ… Row count matches after load: 3925\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8a331a2-cd33-4d3e-851a-cf0394d52b8c"},{"cell_type":"markdown","source":["# team_info.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4704070a-044b-410e-995d-aa226dd6ca80"},{"cell_type":"markdown","source":["Harmonized naming conventions, dropped redundant or null-heavy fields, verified team IDs and abbreviations for validity, and ensured schema and record consistency post-ingestion."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"113a694b-c34b-4469-b24a-acf4e1f4f874"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Step 1 â€” Load raw CSV\n","df_team = pd.read_csv(\"/lakehouse/default/Files/dataset/team_info.csv\")\n","\n","# Step 1b - data inspection \n","print(df_team.head())\n","print(df_team.info())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b4935ca-6083-4251-a8c8-273d041a485b"},{"cell_type":"code","source":["# ============================\n","# team_info. Pipeline\n","# ============================ \n","\n","import pandas as pd\n","import re\n","\n","df_team = pd.read_csv(\"/lakehouse/default/Files/dataset/team_info.csv\")\n","\n","# Step 1 â€” Standardize column names to snake_case\n","def to_snake_case(name):\n","    s1 = \"\"\n","    for i, c in enumerate(name):\n","        if c.isupper() and i != 0 and name[i-1].islower():\n","            s1 += \"_\"\n","        s1 += c.lower()\n","    return s1\n","\n","df_team.columns = [to_snake_case(c) for c in df_team.columns]\n","\n","# Step 2 â€” Drop columns with >85% null values but keep required columns\n","required_cols = ['team_id', 'team_name', 'abbreviation']\n","null_threshold = 0.85\n","df_team = df_team.drop(columns=[c for c in df_team.columns \n","                                if df_team[c].isnull().mean() > null_threshold and c not in required_cols])\n","print(\"âœ… Dropped high-null columns where applicable.\")\n","\n","# Step 3 â€” Remove duplicates\n","df_team = df_team.drop_duplicates()\n","\n","# Step 4 â€” Verify required columns exist\n","missing_cols = [col for col in required_cols if col not in df_team.columns]\n","if missing_cols:\n","    print(f\"âš ï¸ Missing required columns: {missing_cols}\")\n","else:\n","    print(\"âœ… All required columns present.\")\n","\n","# Step 5 â€” Save to Silver layer\n","sdf_team = spark.createDataFrame(df_team)\n","sdf_team.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver_team_info\")\n","\n","# Step 6 â€” Verify row count\n","df_loaded_team = spark.read.table(\"silver_team_info\").toPandas()\n","print(f\"âœ… Rows before load: {len(df_team)}, after load: {len(df_loaded_team)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"ce3146a1-b3d8-40ee-b6d9-365a28822113","normalized_state":"finished","queued_time":"2025-10-18T10:24:33.0989018Z","session_start_time":null,"execution_start_time":"2025-10-18T10:24:33.0999548Z","execution_finish_time":"2025-10-18T10:24:40.9556765Z","parent_msg_id":"27f4c6e6-4f84-45e9-b3fb-e2a14e66c0d8"},"text/plain":"StatementMeta(, ce3146a1-b3d8-40ee-b6d9-365a28822113, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Dropped high-null columns where applicable.\nâœ… All required columns present.\nâœ… Rows before load: 33, after load: 33\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb0ca67b-ddae-4ec4-9c15-a9c6279fdd72"},{"cell_type":"markdown","source":["# game_goal.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c605c204-929f-4e95-9508-56638e50c518"},{"cell_type":"markdown","source":["Renamed columns to consistent snake_case, removed duplicate records, validated goal-related values, ensured strength values were limited to expected categories (Even, Power Play, Short Handed), and verified load consistency."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0987418-d47c-4b3e-9117-88ccd18bb386"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Step 1 â€” Load raw CSV\n","df_goals = pd.read_csv(\"/lakehouse/default/Files/dataset/game_goals.csv\")\n","\n","# Step 1b - data inspection \n","print(df_goals.head())\n","print(df_goals.info())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b6c89be3-c629-4419-a436-b576c567590a"},{"cell_type":"code","source":["\n","# ============================\n","# game_goal. Pipeline\n","# ============================\n","\n","import pandas as pd\n","import re\n","\n","df_goals = pd.read_csv(\"/lakehouse/default/Files/dataset/game_goals.csv\")\n","\n","# Step 1 â€” Standardize column names to snake_case\n","def to_snake_case(name):\n","    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n","    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n","    return name.lower()\n","\n","df_goals.columns = [to_snake_case(c) for c in df_goals.columns]\n","\n","# Step 2 â€” Drop columns with >85% null values but keep required columns\n","required_cols = ['player_id', 'strength', 'game_winning_goal', 'empty_net']\n","null_threshold = 0.85\n","df_goals = df_goals.drop(columns=[c for c in df_goals.columns\n","                                  if df_goals[c].isnull().mean() > null_threshold and c not in required_cols])\n","print(\"âœ… Dropped high-null columns where applicable.\")\n","\n","# Step 3 â€” Remove duplicates\n","df_goals = df_goals.drop_duplicates()\n","\n","# Step 4 â€” Range/value validation\n","# Strength column should have valid categories: 'Even', 'Power Play', 'Short Handed'\n","valid_strength = ['Even', 'Power Play', 'Short Handed']\n","if 'strength' in df_goals.columns:\n","    invalid_strength = df_goals[~df_goals['strength'].isin(valid_strength) & df_goals['strength'].notna()]\n","    if not invalid_strength.empty:\n","        print(\"âš ï¸ Invalid strength rows:\")\n","        print(invalid_strength)\n","\n","# Step 5 â€” Save to Silver layer\n","sdf_goals = spark.createDataFrame(df_goals)\n","sdf_goals.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver_game_goals\")\n","print(\"âœ… Saved to Lakehouse table: silver_game_goals\")\n","\n","# Step 6 â€” Verify row count\n","df_loaded_goals = spark.read.table(\"silver_game_goals\").toPandas()\n","print(f\"âœ… Rows before load: {len(df_goals)}, after load: {len(df_loaded_goals)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"ce3146a1-b3d8-40ee-b6d9-365a28822113","normalized_state":"finished","queued_time":"2025-10-18T10:36:06.5235669Z","session_start_time":null,"execution_start_time":"2025-10-18T10:36:06.5248185Z","execution_finish_time":"2025-10-18T10:36:16.3587698Z","parent_msg_id":"b66e5ccf-74bc-4d77-a6ea-8b2123c41955"},"text/plain":"StatementMeta(, ce3146a1-b3d8-40ee-b6d9-365a28822113, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Dropped high-null columns where applicable.\nâœ… Saved to Lakehouse table: silver_game_goals\nâœ… Rows before load: 133345, after load: 133345\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ded407a9-bfc6-46c2-ae1f-57e96dd92f65"},{"cell_type":"markdown","source":["#"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5b0be9a-8ad5-43a9-a48a-35788107df61"},{"cell_type":"markdown","source":["# game_penalties.csv"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"462dce83-fbcb-49bd-af7d-743635c0ca5a"},{"cell_type":"markdown","source":["Standardized columns, dropped high-null columns, renamed penalty_minutes to pim, validated penalty duration values, removed duplicates, and ensured accurate row counts after loading to the Silver layer."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1735a91-28bd-43b9-b6ca-b90b297d0bc4"},{"cell_type":"code","source":["import pandas as pd\n","\n","# Step 1 â€” Load raw CSV\n","df_penalties = pd.read_csv(\"/lakehouse/default/Files/dataset/game_penalties.csv\")\n","\n","# Step 1b - data inspection \n","print(df_penalties.head())\n","print(df_penalties.info())"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9d6a45ce-410f-4f0f-9f62-91c5072b79f4"},{"cell_type":"code","source":["\n","# ============================\n","# game_penalties. Pipeline\n","# ============================\n","\n","import pandas as pd\n","import re\n","\n","df_penalties = pd.read_csv(\"/lakehouse/default/Files/dataset/game_penalties.csv\")\n","\n","# Step 1 â€” Standardize column names to snake_case\n","def to_snake_case(name):\n","    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n","    name = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n","    return name.lower()\n","\n","df_penalties.columns = [to_snake_case(c) for c in df_penalties.columns]\n","\n","# Step 2 â€” Drop columns with >85% null values but keep required columns\n","required_cols = ['play_id', 'penalty_minutes']\n","null_threshold = 0.85\n","df_penalties = df_penalties.drop(columns=[c for c in df_penalties.columns\n","                                          if df_penalties[c].isnull().mean() > null_threshold and c not in required_cols])\n","print(\"âœ… Dropped high-null columns where applicable.\")\n","\n","# Step 3 â€” Remove duplicates\n","df_penalties = df_penalties.drop_duplicates()\n","\n","# Step 4 â€” Rename penalty_minutes to pim\n","if 'penalty_minutes' in df_penalties.columns:\n","    df_penalties = df_penalties.rename(columns={'penalty_minutes': 'pim'})\n","    print(\"âœ… Renamed 'penalty_minutes' to 'pim'\")\n","\n","# Step 5 â€” Range/value validation\n","invalid_pim = df_penalties[df_penalties['pim'] < 0]\n","if not invalid_pim.empty:\n","    print(\"âš ï¸ Invalid pim rows:\")\n","    print(invalid_pim)\n","\n","# Step 6 â€” Save to Silver layer\n","sdf_penalties = spark.createDataFrame(df_penalties)\n","sdf_penalties.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"silver_game_penalties\")\n","print(\"âœ… Saved to Lakehouse table: silver_game_penalties\")\n","\n","# Step 7 â€” Verify row count\n","df_loaded_penalties = spark.read.table(\"silver_game_penalties\").toPandas()\n","print(f\"âœ… Rows before load: {len(df_penalties)}, after load: {len(df_loaded_penalties)}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"ce3146a1-b3d8-40ee-b6d9-365a28822113","normalized_state":"finished","queued_time":"2025-10-18T10:39:57.5131132Z","session_start_time":null,"execution_start_time":"2025-10-18T10:39:57.514196Z","execution_finish_time":"2025-10-18T10:40:09.3691638Z","parent_msg_id":"062a052c-ebe9-4d34-afd7-0d1413c32e17"},"text/plain":"StatementMeta(, ce3146a1-b3d8-40ee-b6d9-365a28822113, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Dropped high-null columns where applicable.\nâœ… Renamed 'penalty_minutes' to 'pim'\nâœ… Saved to Lakehouse table: silver_game_penalties\nâœ… Rows before load: 229228, after load: 229228\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d5179d58-d548-4dd8-becf-9e2cd385696c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"5b2833da-c51e-4fbf-8136-f97720aa328a","known_lakehouses":[{"id":"8722bc67-5ce0-4be0-8aef-d1e96ab4ff68"},{"id":"5b2833da-c51e-4fbf-8136-f97720aa328a"}],"default_lakehouse_name":"bronze","default_lakehouse_workspace_id":"99622bae-140d-4999-b33c-ee6d3fc53f1e"}}},"nbformat":4,"nbformat_minor":5}